{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cleaning Main Bluebikes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start things off, we will import a few libraries that are pretty universally useful. They will be needed later.\n",
    "We also set the style of visualizations to `matplotlib %inline`, so that this notebook won't be too bloated \n",
    "with interactive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "%matplotlib inline\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette(\"GnBu_d\")\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import math\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset with the data from the bikesharing platform **Bluebikes** from Boston 2018 and take\n",
    "an initial look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data = pd.read_csv('resources/boston_2018.csv')\n",
    "boston_2018_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check for any missing data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boston_2018_data)-len(boston_2018_data.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any data missing. But we just acted like there was and cleaned it regardless, as this\n",
    "code could be transferred to other datasets from Bluebikes and if there would be missing data, we would know what to do.\n",
    "\n",
    "We would choose to drop any missing data, because most of the data contained in the dataset is in nominal scale,\n",
    "like the station names or ids, the user types, the bike ids, or in ordinal scale, like the start and end times.\n",
    "It doesn't make sense to fill such data with averages or previous values, as this would just distort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean = boston_2018_data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when checking the data manually for irregularities, we note that he start and end times start with just seconds as the smallest unit, but somewhere in the data start including milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean.head(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would like to have a comparable and uniform dataset, we just remove the milliseconds from all start and end times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(x):\n",
    "    return x.split('.')[0]\n",
    "\n",
    "boston_2018_data_clean['start_time'] = boston_2018_data_clean['start_time'].apply(lambda x: clean_date(x))\n",
    "boston_2018_data_clean['end_time'] = boston_2018_data_clean['end_time'].apply(lambda x: clean_date(x))\n",
    "boston_2018_data_clean.head(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There don't seem to be any more irregularities after that, so we can move on to preparing the data for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preparing Bluebikes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the dataset, we need to extract a few important pieces of data. We would like to be able to group our data by date, weekday, hour and month, so we need to extract that out of one of the time columns.\n",
    "\n",
    "We choose the start_time, so the values extracted are from the start of the bike trip. It is important to keep that in mind.\n",
    "\n",
    "**Important information:** These apply methods where the data is extracted can take a few minutes, due to them having\n",
    "to go through the entire dataset. Patience is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    return datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").date()\n",
    "\n",
    "def get_weekday(x):\n",
    "    return datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").weekday()\n",
    "\n",
    "def get_hour(x):\n",
    "    return datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour\n",
    "\n",
    "def get_month(x):\n",
    "    return datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean.reset_index(drop=True)\n",
    "boston_2018_data_clean['date'] = boston_2018_data_clean['start_time'].apply(lambda x: get_date(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean['weekday'] = boston_2018_data_clean['start_time'].apply(lambda x: get_weekday(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean['hour'] = boston_2018_data_clean['start_time'].apply(lambda x: get_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean['month'] = boston_2018_data_clean['start_time'].apply(lambda x: get_month(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it has worked and we now have extra columns for the date, weekday, hour and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cleaning Bluebikes bike stations dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a lot of data when and how long bike trips were, but we would also like to now where they started and where they ended. We already have the start station and end station names and ids, but we can't really do anything with that,\n",
    "as we do not know where that is. That is why we also use the Bluebikes stations dataset to get some more geographical information.\n",
    "\n",
    "We start by doing the same thing as with our initial dataset, just loading the data and taking a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations = pd.read_csv('resources/current_bluebikes_stations.csv')\n",
    "bike_stations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the columns are a bit off, as the first line of the dataset described when the data was last updated and \n",
    "the second line are the column headers. We fix that by just dropping the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations.columns = bike_stations.iloc[0]\n",
    "bike_stations.drop(bike_stations.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the first dataset, we check for missing data and would drop it, but there is none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bike_stations) - len(bike_stations.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we have the name of the station, we can use that to assign the information from this dataset to our initial dataset by merging the data on the name column using `pd.merge`. We want to transfer both the start and the end location to a new combined dataset.\n",
    "\n",
    "After that, we inspect it to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_start_geodata = pd.merge(boston_2018_data_clean, bike_stations[['Name','Latitude', 'Longitude', 'District']], left_on=\"start_station_name\", right_on=\"Name\", how=\"left\")\n",
    "boston_2018_start_geodata = boston_2018_start_geodata.rename(columns = {'Latitude': 'latitude_start', 'Longitude': 'longitude_start', \"District\": \"district_start\"})\n",
    "boston_2018_start_geodata['start_coordinates'] = list(zip(boston_2018_start_geodata[\"latitude_start\"],boston_2018_start_geodata[\"longitude_start\"]))\n",
    "del boston_2018_start_geodata['Name']\n",
    "\n",
    "boston_2018_trip_geodata = pd.merge(boston_2018_start_geodata, bike_stations[['Name','Latitude', 'Longitude', 'District']], left_on=\"end_station_name\", right_on=\"Name\", how=\"left\")\n",
    "boston_2018_trip_geodata = boston_2018_trip_geodata.rename(columns = {'Latitude': 'latitude_end', 'Longitude': 'longitude_end', \"District\": \"district_end\"})\n",
    "boston_2018_trip_geodata['end_coordinates'] = list(zip(boston_2018_trip_geodata[\"latitude_end\"],boston_2018_trip_geodata[\"longitude_end\"]))\n",
    "del boston_2018_trip_geodata['Name']\n",
    "\n",
    "boston_2018_trip_geodata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have worked, but we can already spot a NaN value in the first five values. This is due to not every station name in the initial dataset being compatible with a station name in the station dataset. Looking at the amount of missing values, we need to do some cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boston_2018_trip_geodata)-len(boston_2018_trip_geodata.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just drop any missing data, as we can not use it for geographical purposes and it makes no sense to have a combined dataset when one half is missing. We can always use the main dataset for those purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_trip_geodata_clean = boston_2018_trip_geodata.dropna()\n",
    "len(boston_2018_trip_geodata_clean)-len(boston_2018_trip_geodata_clean.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cleaning Boston weather dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems very likely that the weather also plays a big role in a bike sharing platform. Assuming that people usually don't like to ride a bike in rain or at freezing temperatures seems pretty intuitive.\n",
    "That is why we also include a weather dataset for boston.\n",
    "\n",
    "We proceed as we did with the previous datasets and load it first to look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_weather = pd.read_csv('resources/weather_hourly_boston.csv')\n",
    "boston_weather.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be anything special in this dataset, so we continue cleaning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boston_weather) - len(boston_weather.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the missing values, because they don't provide any valuable information and can be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_weather_clean = boston_weather.dropna()\n",
    "len(boston_weather_clean) - len(boston_weather_clean.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare to merge it with the main dataset. To do that, we extract the hour and the date from the date_time column, as we have exactly the same columns in our main dataset and these two form a unique identifier for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_weather_clean.reset_index(drop=True)\n",
    "pd.options.mode.chained_assignment = None\n",
    "boston_weather_clean['date'] = boston_weather_clean['date_time'].apply(lambda x:get_date(x))\n",
    "boston_weather_clean['hour'] = boston_weather_clean['date_time'].apply(lambda x:get_hour(x))\n",
    "boston_weather_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then just merge the data with our main dataset on the date and hour columns using `pd.merge` and take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_trip_geodata_with_temp = pd.merge(boston_2018_trip_geodata_clean, \n",
    "                                              boston_weather_clean[['max_temp','min_temp', 'precip', 'date', 'hour']],\n",
    "                                              on=['date', 'hour'], how=\"left\")\n",
    "boston_2018_trip_geodata_with_temp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the geographical data, we need to take a look at missing data afterwards, due to the fact that some days or hours may be missing in the temperature dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boston_2018_trip_geodata_with_temp)-len(boston_2018_trip_geodata_with_temp.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the same as with the geographical dataset, missing data doesn't provide any value, so we just drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_trip_geodata_with_temp_clean = boston_2018_trip_geodata_with_temp.dropna()\n",
    "len(boston_2018_trip_geodata_with_temp_clean)-len(boston_2018_trip_geodata_with_temp_clean.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have several datasets containing much information and can move on to visualize it to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Descriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Temporal Demand Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of our project we want to know how many bikes where borrowed in one day, one week and one year. For this we plot three different visualizations to see how it changed through the times we want to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with weekday names \n",
    "weeday_dict = {0:\"Mon\",1:\"Tue\",2:\"Wed\",3:\"Thu\",4:\"Fri\",5:\"Sat\",6:\"Sun\"}\n",
    "\n",
    "boston_2018_data_clean[\"Weekday_name\"] = boston_2018_data_clean[\"weekday\"].apply(lambda x: weeday_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better look at the x axis we create a column with weekday names instead of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with month names\n",
    "month_dict = {1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n",
    "\n",
    "boston_2018_data_clean[\"month_name\"] = boston_2018_data_clean[\"month\"].apply(lambda x: month_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons we also create a column with months names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column with an ascending index of each row\n",
    "Bike_anzahl=boston_2018_data_clean.index\n",
    "\n",
    "boston_2018_data_clean[\"Bike_Anzahl\"]= Bike_anzahl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a column with an ascending index of each row to differentiate every single rent of the bike through the year. In the next step we use the groupby Statement to order every rent to its date and hour. After this we can plot the quantity of borrowed bikes with a boxplot to see variance and the minimum/maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a boxplot of borrowed bikes per day\n",
    "boston_tempered_hour = boston_2018_data_clean.groupby([\"date\",\"hour\"])[\"Bike_Anzahl\"].nunique()\n",
    "boston_tempered_hour= pd.DataFrame(boston_tempered_hour)\n",
    "fig,ax = plt.subplots(figsize=(12,4)) \n",
    "\n",
    "sns.boxplot(x=boston_tempered_hour.index.get_level_values(1), y=boston_tempered_hour[\"Bike_Anzahl\"],ax=ax)\n",
    "ax.set_title(\"Bikes borrowed per day\",fontsize=16)\n",
    "plt.ylabel('number of borrowed bikes')\n",
    "#plt.savefig(\"daydemand.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same boxplot with week instead of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a boxplot of borrowed bikes per week\n",
    "boston_tempered_weekday = boston_2018_data_clean.groupby([\"date\",\"Weekday_name\"])[\"Bike_Anzahl\"].nunique()\n",
    "boston_tempered_weekday= pd.DataFrame(boston_tempered_weekday)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,4)) \n",
    "\n",
    "sns.boxplot(x=boston_tempered_weekday.index.get_level_values(1), y=boston_tempered_weekday[\"Bike_Anzahl\"],ax=ax)\n",
    "ax.set_title(\"Bikes borrowed per week\",fontsize=16)\n",
    "plt.xlabel('weekday')\n",
    "plt.ylabel('number of borrowed bikes')\n",
    "#plt.savefig(\"weekdemand.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the year demand, we create a column with 1 in every row to count the values for every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with index 1 in every row\n",
    "month_dict2 = {1:\"1\",2:\"1\",3:\"1\",4:\"1\",5:\"1\",6:\"1\",7:\"1\",8:\"1\",9:\"1\",10:\"1\",11:\"1\",12:\"1\"}\n",
    "\n",
    "boston_2018_data_clean[\"Index\"] = boston_2018_data_clean[\"month\"].apply(lambda x: month_dict2[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the number of borrowed bikes for every month\n",
    "Jan = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==1].groupby(\"Index\").count()\n",
    "Feb = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==2].groupby(\"Index\").count()\n",
    "Mar = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==3].groupby(\"Index\").count()\n",
    "Apr = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==4].groupby(\"Index\").count()\n",
    "May = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==5].groupby(\"Index\").count()\n",
    "Jun = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==6].groupby(\"Index\").count()\n",
    "Jul = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==7].groupby(\"Index\").count()\n",
    "Aug = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==8].groupby(\"Index\").count()\n",
    "Sep = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==9].groupby(\"Index\").count()\n",
    "Oct = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==10].groupby(\"Index\").count()\n",
    "Nov = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==11].groupby(\"Index\").count()\n",
    "Dec = boston_2018_data_clean[boston_2018_data_clean[\"month\"]==12].groupby(\"Index\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the claculated table\n",
    "Data = {\"Jan\": Jan['month'],\n",
    "      \"Feb\": Feb['month'],\n",
    "       \"Mar\": Mar['month'],\n",
    "        \"Apr\": Apr['month'],\n",
    "        \"May\": May['month'],\n",
    "        \"Jun\": Jun['month'],\n",
    "        \"Jul\": Jul['month'],\n",
    "        \"Aug\": Aug['month'],\n",
    "        \"Sep\": Sep['month'],\n",
    "        \"Oct\": Oct['month'],\n",
    "        \"Nov\": Nov['month'],\n",
    "        \"Dec\": Dec['month']\n",
    "       }\n",
    "\n",
    "Year = pd.DataFrame(Data)\n",
    "print(Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new dataframe with the counted values for every month to plot it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bar chart\n",
    "datayear = {'month': ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov', 'Dec'],\n",
    "        'quantity': [40932,62817,62985,98194,178865,205359,242916,236076,236182,200100,121419,81961]\n",
    "        }\n",
    "\n",
    "Year2 = pd.DataFrame(datayear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the fact that we used data for one year, we plotted the demand with an barchart because of missing years to compare in a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=Year2.plot.bar(x='month', y='quantity', rot=0, figsize=(14,6))\n",
    "ax.set_title(\"Bikes borrowed in year 2018\",fontsize=16)\n",
    "plt.ylabel('number of borrowed bikes', fontsize=14)\n",
    "plt.xlabel('month', fontsize=14)\n",
    "#plt.savefig(\"yeardemand.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this part we count the different months to differentiate them into seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bar chart for seasons\n",
    "data_season = {'season': ['Spring','Summer','Autumn','Winter'],\n",
    "        'quantity': [340044,684351,557701,185710]\n",
    "        }\n",
    "\n",
    "season = pd.DataFrame(data_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=season.plot.bar(x='season', y='quantity', rot=0, figsize=(14,6))\n",
    "ax.set_title(\"Bikes borrowed in seasons\",fontsize=16)\n",
    "plt.ylabel('number of borrowed bikes', fontsize=14)\n",
    "plt.xlabel('season', fontsize=14)\n",
    "plt.savefig(\"seasondemand.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Geographical Demand Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, sharey=True, figsize=(20, 10))\n",
    "sns.countplot(x=\"district_start\",data=boston_2018_trip_geodata_clean, ax=axes[0], order=boston_2018_trip_geodata_clean['district_start'].value_counts().index)\n",
    "sns.countplot(x=\"district_end\",data=boston_2018_trip_geodata_clean, ax=axes[1], order=boston_2018_trip_geodata_clean['district_start'].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_trip_geodata_clean[\"Coordinates\"] = list(zip(boston_2018_trip_geodata_clean[\"latitude_start\"],boston_2018_trip_geodata_clean[\"longitude_start\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_list = boston_2018_trip_geodata_clean[\"bike_id\"].unique()\n",
    "selected_bike_ID = random.choice(bike_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new map\n",
    "\n",
    "demand_heat_map = folium.Map(location=(42.337510, -71.067966),tiles='Stamen Toner', \n",
    "                       zoom_start=12, control_scale=True, max_zoom=20)\n",
    "\n",
    "# add heat map\n",
    "\n",
    "demand_heat_map.add_child(plugins.HeatMap(boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"bike_id\"]==selected_bike_ID][\"Coordinates\"], radius=20))\n",
    "\n",
    "#add tilelayers \n",
    "\n",
    "folium.TileLayer('stamentoner').add_to(demand_heat_map)\n",
    "folium.TileLayer('cartodbpositron').add_to(demand_heat_map)\n",
    "folium.TileLayer('openstreetmap').add_to(demand_heat_map)\n",
    "folium.LayerControl().add_to(demand_heat_map)\n",
    "\n",
    "demand_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows in which 'district_start' is 'specific one'\n",
    "count_cambridge = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Cambridge\" else False, axis = 1)\n",
    "count_somerville = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Somerville\" else False, axis = 1)\n",
    "count_boston = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Boston\" else False, axis = 1)\n",
    "count_brookline = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Brookline\" else False, axis = 1)\n",
    "\n",
    "# rows in which 'district_end' is 'specific one'\n",
    "count_cambridge = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Cambridge\" else False, axis = 1)\n",
    "count_somerville = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Somerville\" else False, axis = 1)\n",
    "count_boston = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Boston\" else False, axis = 1)\n",
    "count_brookline = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Brookline\" else False, axis = 1)\n",
    "\n",
    "# Count number of True in the series\n",
    "num_rows_c = len(count_cambridge[count_cambridge == True].index)\n",
    "num_rows_s = len(count_somerville[count_somerville == True].index)\n",
    "num_rows_bo = len(count_boston[count_boston == True].index)\n",
    "num_rows_br = len(count_brookline[count_brookline == True].index)\n",
    "\n",
    "num_rows_c0 = len(count_cambridge[count_cambridge == True].index)\n",
    "num_rows_s1 = len(count_somerville[count_somerville == True].index)\n",
    "num_rows_bo2 = len(count_boston[count_boston == True].index)\n",
    "num_rows_br3 = len(count_brookline[count_brookline == True].index)\n",
    "\n",
    "# point = boston_2018_trip_geodata_clean.apply(lambda x: get_district_end(x))\n",
    "\n",
    "data = {'Districts':  ['Cambridge', 'Boston', 'Brookline', 'Somerville'],\n",
    "        'Number': [num_rows_c0 + num_rows_c, num_rows_bo2 + num_rows_bo, num_rows_br3 + num_rows_br, num_rows_s1 + num_rows_s]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['Districts','Number'])\n",
    "\n",
    "g = sns.barplot(x=\"Districts\", y=\"Number\", data=df, palette=\"OrRd\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new map\n",
    "\n",
    "s_demand_heat_map = folium.Map(location=(42.337510, -71.067966),  tiles='Stamen Toner', \n",
    "                       zoom_start=12, control_scale=True, max_zoom=20)\n",
    "\n",
    "# add heat map\n",
    "\n",
    "s_demand_heat_map.add_child(plugins.HeatMap(boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"bike_id\"]==selected_bike_ID][\"Coordinates\"], radius=20))\n",
    "\n",
    "#add tilelayers \n",
    "\n",
    "folium.TileLayer('stamentoner').add_to(s_demand_heat_map)\n",
    "folium.TileLayer('cartodbpositron').add_to(s_demand_heat_map)\n",
    "folium.TileLayer('openstreetmap').add_to(s_demand_heat_map)\n",
    "folium.LayerControl().add_to(s_demand_heat_map)\n",
    "\n",
    "s_demand_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows in which 'district_start' is 'specific one'\n",
    "count_cambridge = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Cambridge\" else False, axis = 1)\n",
    "count_somerville = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Somerville\" else False, axis = 1)\n",
    "count_boston = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Boston\" else False, axis = 1)\n",
    "count_brookline = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_start'] == \"Brookline\" else False, axis = 1)\n",
    "\n",
    "# Count number of True in the series\n",
    "num_rows_c0 = len(count_cambridge[count_cambridge == True].index)\n",
    "num_rows_s1 = len(count_somerville[count_somerville == True].index)\n",
    "num_rows_bo2 = len(count_boston[count_boston == True].index)\n",
    "num_rows_br3 = len(count_brookline[count_brookline == True].index)\n",
    "\n",
    "data = {'District_start':  ['Cambridge', 'Boston', 'Brookline', 'Somerville'],\n",
    "        'Number': [num_rows_c0, num_rows_bo2, num_rows_br3, num_rows_s1]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['District_start','Number'])\n",
    "print(df)\n",
    "\n",
    "sns.barplot(x=\"District_start\", y=\"Number\", data=df, palette=\"OrRd\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new map\n",
    "\n",
    "e_demand_heat_map = folium.Map(location=(42.337510, -71.067966),  tiles='Stamen Terrain', \n",
    "                       zoom_start=12, control_scale=True, max_zoom=20)\n",
    "\n",
    "# add heat map\n",
    "\n",
    "e_demand_heat_map.add_child(plugins.HeatMap(boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"bike_id\"]==selected_bike_ID][\"Coordinates\"], radius=20))\n",
    "\n",
    "#add tilelayers \n",
    "\n",
    "folium.TileLayer('stamentoner').add_to(e_demand_heat_map)\n",
    "folium.TileLayer('cartodbpositron').add_to(e_demand_heat_map)\n",
    "folium.TileLayer('openstreetmap').add_to(e_demand_heat_map)\n",
    "folium.LayerControl().add_to(e_demand_heat_map)\n",
    "\n",
    "e_demand_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows in which 'district_end' is 'specific one'\n",
    "count_cambridge = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Cambridge\" else False, axis = 1)\n",
    "count_somerville = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Somerville\" else False, axis = 1)\n",
    "count_boston = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Boston\" else False, axis = 1)\n",
    "count_brookline = boston_2018_trip_geodata_clean.apply(lambda x : True\n",
    "            if x['district_end'] == \"Brookline\" else False, axis = 1)\n",
    "\n",
    "# Count number of True in the series\n",
    "num_rows_c = len(count_cambridge[count_cambridge == True].index)\n",
    "num_rows_s = len(count_somerville[count_somerville == True].index)\n",
    "num_rows_bo = len(count_boston[count_boston == True].index)\n",
    "num_rows_br = len(count_brookline[count_brookline == True].index)\n",
    "\n",
    "data = {'District_end':  ['Cambridge', 'Boston', 'Brookline', 'Somerville'],\n",
    "        'Number': [num_rows_c, num_rows_bo, num_rows_br, num_rows_s]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame (data, columns = ['District_end','Number'])\n",
    "print(df) \n",
    "\n",
    "sns.barplot(x=\"District_end\", y=\"Number\", data=df, palette=\"OrRd\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare start and end\n",
    "Here you can see the difference between tour starts and tour ends in the districts. For example, a positive value means that there were more tour starts than tour ends in the respective district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = {\"District\": [\"Cambridge\", \"Boston\", \"Brookline\", \"Somerville\"],\n",
    "                \"Difference\": [(num_rows_c0-num_rows_c), (num_rows_bo2-num_rows_bo), (num_rows_br3-num_rows_br), (num_rows_s1-num_rows_s)]\n",
    "}\n",
    "diff = pd.DataFrame (difference, columns = [\"District\",'Difference'])\n",
    "print(diff) \n",
    "\n",
    "sns.barplot(x=\"District\", y=\"Difference\", data=diff, palette=\"OrRd\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choropleth Map of Boston\n",
    "### Convert lat and long coordinates into zipcodes  \n",
    "This step takes much time. For 1000 conversions about 5 minutes. We convert the lang and long into the zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#longlat = boston_2018_trip_geodata_clean\n",
    "geolocator = geopy.Nominatim(user_agent=\"check_1\")\n",
    "\n",
    "\n",
    "    \n",
    "def get_zip_code(x):\n",
    "    location = geolocator.reverse(\"{}, {}\".format(x['latitude_start'],x['longitude_start']))\n",
    "    try:\n",
    "        return (location.raw['address']['postcode'])\n",
    "    except:\n",
    "        return (None, None)\n",
    "boston_2018_trip_geodata_clean['zipcode'] = boston_2018_trip_geodata_clean.head(1000).apply(lambda x: get_zip_code(x), axis = 1)\n",
    "print(boston_2018_trip_geodata_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleth map of greater boston \n",
    "In this visualization you can see boston and its surrounding districts. Here you can read and recognise the demand much better than in the heatmaps before. For this, the individual incidents are no longer precisely localised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the boundaries in the map\n",
    "boston_and_greater = r'boston_and_greater.json'\n",
    "\n",
    "# calculating total number of incidents per district\n",
    "data2 = pd.DataFrame(boston_2018_trip_geodata_clean['bike_id'].value_counts().astype(float))\n",
    "data2.to_json('map.json')\n",
    "data2 = data2.reset_index()\n",
    "data2.columns = ['zipcode', 'bike_id']\n",
    "  \n",
    "# creation of the choropleth\n",
    "simple_map = folium.Map(location=(42.337510, -71.067966), zoom_start=12)\n",
    "\n",
    "#Boston and greater\n",
    "folium.Choropleth(\n",
    "    geo_data = boston_and_greater,\n",
    "    name=\"Boston_and_greater\",\n",
    "    data=boston_2018_trip_geodata_clean,\n",
    "    columns=[\"zipcode\", 'bike_id'],\n",
    "    key_on='feature.properties.ZIP5',\n",
    "    fill_color=\"YlOrRd\",\n",
    "    fill_opacity=0.7,\n",
    "   line_opacity=1.0,\n",
    ").add_to(simple_map)\n",
    "\n",
    "#add tilelayers \n",
    "folium.TileLayer('openstreetmap').add_to(simple_map)\n",
    "folium.TileLayer('Stamen Terrain').add_to(simple_map)\n",
    "folium.TileLayer('cartodbpositron').add_to(simple_map)\n",
    "folium.LayerControl().add_to(simple_map)\n",
    "               \n",
    "display(simple_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this map you can choose the districts you want to see through the layers and the layer control. For example, if you only want to see the demand in Brookline and Cambridge, then only pick these two in the layer control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the boundaries in the map\n",
    "boston_and_greater = r'boston_and_greater.json'\n",
    "Boston = r'ZIP_Codes.geojson'\n",
    "Cambridge = r'Cambridge_Zipcodes.geojson'\n",
    "Somerville = r'somerville.json'\n",
    "Brookline = r'brookline.json'\n",
    "\n",
    "# calculating total number of incidents per district\n",
    "data2 = pd.DataFrame(boston_2018_trip_geodata_clean['bike_id'].value_counts().astype(float))\n",
    "data2.to_json('map.json')\n",
    "data2 = data2.reset_index()\n",
    "data2.columns = ['zipcode', 'bike_id']\n",
    "  \n",
    "# creation of the choropleth\n",
    "whole_map = folium.Map(location=(42.337510, -71.067966), zoom_start=12)\n",
    "\n",
    "# Boston\n",
    "folium.Choropleth(\n",
    "    geo_data = Boston,\n",
    "    name=\"Boston\",\n",
    "    data=boston_2018_trip_geodata_clean,\n",
    "    columns=[\"zipcode\", 'bike_id'],\n",
    "    key_on='feature.properties.ZIP5',\n",
    "    fill_color=\"Blues\",\n",
    "    fill_opacity=0.7,\n",
    "   line_opacity=1.0,\n",
    "    legend_name=\"Usage per district\",\n",
    ").add_to(whole_map)\n",
    "\n",
    "# Somerville\n",
    "folium.Choropleth(\n",
    "    geo_data = Somerville,\n",
    "    name=\"Somerville\",\n",
    "    data=boston_2018_trip_geodata_clean,\n",
    "    columns=[\"zipcode\", 'bike_id'],\n",
    "    key_on='feature.properties.ZIP5',\n",
    "    fill_color=\"Greens\",\n",
    "    fill_opacity=0.7,\n",
    "   line_opacity=1.0,\n",
    "    legend_name=\"Usage per district\",\n",
    ").add_to(whole_map)\n",
    "\n",
    "# Cambridge\n",
    "folium.Choropleth(\n",
    "    geo_data = Cambridge,\n",
    "    name=\"Cambridge\",\n",
    "    data=boston_2018_trip_geodata_clean,\n",
    "    columns=[\"zipcode\", 'bike_id'],\n",
    "    key_on='feature.properties.ZIP_CODE',\n",
    "    fill_color=\"Reds\",\n",
    "    fill_opacity=0.7,\n",
    "   line_opacity=1.0,\n",
    ").add_to(whole_map)\n",
    "\n",
    "# Brookline\n",
    "folium.Choropleth(\n",
    "    geo_data = Brookline,\n",
    "    name=\"Brookline\",\n",
    "    data=boston_2018_trip_geodata_clean,\n",
    "    columns=[\"zipcode\", 'bike_id'],\n",
    "    key_on='feature.properties.ZIP5',\n",
    "    fill_color=\"Purples\",\n",
    "    fill_opacity=0.7,\n",
    "   line_opacity=1.0,\n",
    ").add_to(whole_map)\n",
    "\n",
    "        \n",
    "#add tilelayers \n",
    "folium.TileLayer('openstreetmap').add_to(whole_map)\n",
    "folium.TileLayer('Stamen Terrain').add_to(whole_map)\n",
    "folium.TileLayer('cartodbpositron').add_to(whole_map)\n",
    "folium.LayerControl().add_to(whole_map)\n",
    "               \n",
    "display(whole_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Key Performance Indicators (KPIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the duration of a trip, we first convert start_tkme and end_time, which are strings, into datetime objects.\n",
    "This makes getting time differences easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime(x):\n",
    "    return datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "boston_2018_data_clean['datetime_start'] = boston_2018_data_clean['start_time'].apply(lambda x: get_datetime(x))\n",
    "boston_2018_data_clean['datetime_end'] = boston_2018_data_clean['end_time'].apply(lambda x: get_datetime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we now have the datetime objects, we can easily get the time difference from start time to end time to\n",
    "get the duration of the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_2018_data_clean['duration'] = boston_2018_data_clean['datetime_end'] - boston_2018_data_clean['datetime_start']\n",
    "boston_2018_data_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert that timedelta object to a simple number of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minutes(x):\n",
    "    return (x.total_seconds())/60\n",
    "\n",
    "boston_2018_data_clean['minutes'] = boston_2018_data_clean['duration'].apply(lambda x: get_minutes(x))\n",
    "boston_2018_data_clean.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can already visualize it. We use boxplots to show the medians and also visualize the deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 1, figsize=(15, 5))\n",
    "\n",
    "sns.boxplot(x = \"hour\", y = \"minutes\", data= boston_2018_data_clean, ax=axes, showfliers=False)\n",
    "axes.set_title(\"Median duration of trip per hour throughout the year (KPI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the total number of borrowed bikes for every hour in the year\n",
    "boston_tempered_hour = boston_2018_data_clean.groupby([\"Index\",\"hour\"])[\"Bike_Anzahl\"].nunique()\n",
    "boston_tempered_hour = pd.DataFrame(boston_tempered_hour)\n",
    "fig,ax = plt.subplots(figsize=(12,4))\n",
    "sns.barplot(x=boston_tempered_hour.index.get_level_values(1), y=boston_tempered_hour[\"Bike_Anzahl\"],ax=ax)\n",
    "ax.set_title(\"total number of bicycles per hour in the year\",fontsize=16)\n",
    "plt.ylabel('number of borrowed bikes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of the fleet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the proportion of the fleet first we calculated the fleet size of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the fleet size\n",
    "fleet_size = len(boston_2018_data_clean[\"bike_id\"].unique())\n",
    "fleet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the number of different bike IDs borrowed every hour\n",
    "boston_tempered_hour123 = boston_2018_data_clean.groupby([\"date\",\"hour\"])[\"bike_id\"].nunique()\n",
    "boston_tempered_hour123= pd.DataFrame(boston_tempered_hour123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_tempered_hour123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataframe we see the number of different bike ids borrowed every hour. In the next step we divide it with the fleet size to get the proportion in percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a new column with the calculated proportion of the fleet borrowed every hour\n",
    "boston_tempered_hour123[\"bike_id2\"]=(boston_tempered_hour123[\"bike_id\"]/4045)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_tempered_hour123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see a third column with percent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot it in a boxplot to the proportion of the fleet borrowed per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the calculated proportion of the fleet borrowed per day\n",
    "fig,ax = plt.subplots(figsize=(12,4)) \n",
    "sns.boxplot(x=boston_tempered_hour123.index.get_level_values(1), y=boston_tempered_hour123[\"bike_id2\"],ax=ax)\n",
    "ax.set_title(\"Proportion of the fleet borrowed per day\",fontsize=16)\n",
    "plt.ylabel('Proportion of the fleet in percent')\n",
    "plt.savefig(\"dayproportion.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also can be interesting to calculate the proportion of the fleet borrowed per week and the whole year. Because of this we plot two another visualizations for week and year in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the number of different bike IDs borrowed in a week\n",
    "boston_tempered_week1= boston_2018_data_clean.groupby([\"date\",\"Weekday_name\"])[\"bike_id\"].nunique()\n",
    "boston_tempered_week1= pd.DataFrame(boston_tempered_week1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a new column with the calculated proportion of the fleet borrowed in one week\n",
    "boston_tempered_week1[\"bike_id2\"]=(boston_tempered_week1[\"bike_id\"]/4045)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the calculated proportion of the fleet per week\n",
    "fig,ax = plt.subplots(figsize=(12,4)) \n",
    "sns.boxplot(x=boston_tempered_week1.index.get_level_values(1), y=boston_tempered_week1[\"bike_id2\"],ax=ax)\n",
    "ax.set_title(\"Proportion of the fleet borrowed per week\",fontsize=16)\n",
    "plt.ylabel('Proportion of the fleet in percent')\n",
    "plt.xlabel('weekday')\n",
    "plt.savefig(\"weekproportion.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the number of different bike IDs borrowed every month\n",
    "boston_tempered_year1= boston_2018_data_clean.groupby([\"month\"])[\"bike_id\"].nunique()\n",
    "boston_tempered_year1= pd.DataFrame(boston_tempered_year1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a new column with the calculated proportion of the fleet borrowed every month\n",
    "boston_tempered_year1[\"proportion\"]=(boston_tempered_year1[\"bike_id\"]/4045)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a third column for a better visualization of the x axis\n",
    "month_name = {1491:\"Jan\",1560:\"Feb\",1544:\"Mar\",1501:\"Apr\",2668:\"May\",2098:\"Jun\",2123:\"Jul\",2932:\"Aug\",2388:\"Sep\",2382:\"Oct\",2243:\"Nov\",2374:\"Dec\"}\n",
    "\n",
    "boston_tempered_year1[\"month1\"] = boston_tempered_year1[\"bike_id\"].apply(lambda x: month_name[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a barchart to see the proportion of every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the calculated proportion of the fleet borrowed every month\n",
    "fig,ax = plt.subplots(figsize=(12,4)) \n",
    "ax=boston_tempered_year1.plot.bar(x='month1', y='proportion', rot=0, figsize=(14,6))\n",
    "ax.set_title(\"Proportion of the fleet borrowed per year\",fontsize=16)\n",
    "plt.ylabel('Proportion of the fleet in percent', fontsize=14)\n",
    "plt.xlabel('month', fontsize=14)\n",
    "plt.savefig(\"yearproportion.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subscriber-customer-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most popular stations\n",
    "### Most Popular Start Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start & end over the whole year\n",
    "start_station_list = boston_2018_trip_geodata_clean[\"start_station_name\"].unique()\n",
    "end_station_list = boston_2018_trip_geodata_clean[\"end_station_name\"].unique()\n",
    "\n",
    "freq1 = boston_2018_trip_geodata_clean['start_station_name'].value_counts()  \n",
    "freq2 = boston_2018_trip_geodata_clean['end_station_name'].value_counts()\n",
    "\n",
    "freq1pd = pd.DataFrame({\n",
    "                        'anzahl': freq1\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts the frequency of the station names during a specific hour over the whole year\n",
    "freq_h = pd.DataFrame({\n",
    "    0: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 0]['start_station_name'].value_counts(),\n",
    "    1: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 1]['start_station_name'].value_counts(), \n",
    "    2: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 2]['start_station_name'].value_counts(), \n",
    "    3: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 3]['start_station_name'].value_counts(), \n",
    "    4: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 4]['start_station_name'].value_counts(), \n",
    "    5: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 5]['start_station_name'].value_counts(), \n",
    "    6: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 6]['start_station_name'].value_counts(), \n",
    "    7: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 7]['start_station_name'].value_counts(), \n",
    "    8: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 8]['start_station_name'].value_counts(), \n",
    "    9: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 9]['start_station_name'].value_counts(), \n",
    "    10: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 10]['start_station_name'].value_counts(), \n",
    "    11: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 11]['start_station_name'].value_counts(), \n",
    "    12: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 12]['start_station_name'].value_counts(), \n",
    "    13: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 13]['start_station_name'].value_counts(), \n",
    "    14: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 14]['start_station_name'].value_counts(), \n",
    "    15: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 15]['start_station_name'].value_counts(), \n",
    "    16: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 16]['start_station_name'].value_counts(), \n",
    "    17: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 17]['start_station_name'].value_counts(), \n",
    "    18: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 18]['start_station_name'].value_counts(), \n",
    "    19: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 19]['start_station_name'].value_counts(), \n",
    "    20: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 20]['start_station_name'].value_counts(), \n",
    "    21: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 21]['start_station_name'].value_counts(), \n",
    "    22: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 22]['start_station_name'].value_counts(), \n",
    "    23: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 23]['start_station_name'].value_counts()\n",
    "}); \n",
    "freq_h = freq_h.fillna(0)\n",
    "freq_h.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum per row\n",
    "most_pop = np.empty(24)\n",
    "freq_h['sum'] = 0\n",
    "\n",
    "for j in range(246):\n",
    "    for i in range(24):\n",
    "        most_pop[i] = freq_h[i][j]\n",
    "    freq_h['sum'][j] = most_pop.sum()\n",
    "\n",
    "freq_h.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the most popular start station\n",
    "most_freq_h = freq_h[freq_h['sum'] > 15000]\n",
    "most_freq_h.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "figure, axes = plt.subplots(24, 1, figsize=(150, 200))\n",
    "\n",
    "sns.barplot(x = most_freq_h.index, y=0, data=most_freq_h, ax=axes[0], palette=\"OrRd\")\n",
    "axes[0].set_title(\"Most popular start stations\")\n",
    "sns.barplot(x = most_freq_h.index , y=1, data=most_freq_h, ax=axes[1], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=2, data=most_freq_h, ax=axes[2], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=3, data=most_freq_h, ax=axes[3], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=4, data=most_freq_h, ax=axes[4], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=5, data=most_freq_h, ax=axes[5], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=6, data=most_freq_h, ax=axes[6], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=7, data=most_freq_h, ax=axes[7], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=8, data=most_freq_h, ax=axes[8], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=9, data=most_freq_h, ax=axes[9], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=10, data=most_freq_h, ax=axes[10], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=11, data=most_freq_h, ax=axes[11], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=12, data=most_freq_h, ax=axes[12], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=13, data=most_freq_h, ax=axes[13], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=14, data=most_freq_h, ax=axes[14], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=15, data=most_freq_h, ax=axes[15], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=16, data=most_freq_h, ax=axes[16], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=17, data=most_freq_h, ax=axes[17], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=18, data=most_freq_h, ax=axes[18], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=19, data=most_freq_h, ax=axes[19], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=20, data=most_freq_h, ax=axes[20], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=21, data=most_freq_h, ax=axes[21], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=22, data=most_freq_h, ax=axes[22], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_h.index , y=23, data=most_freq_h, ax=axes[23], palette=\"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular End Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_z = pd.DataFrame({\n",
    "    0: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 0]['end_station_name'].value_counts(),\n",
    "    1: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 1]['end_station_name'].value_counts(), \n",
    "    2: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 2]['end_station_name'].value_counts(), \n",
    "    3: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 3]['end_station_name'].value_counts(),\n",
    "    4: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 4]['end_station_name'].value_counts(), \n",
    "    5: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 5]['end_station_name'].value_counts(), \n",
    "    6: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 6]['end_station_name'].value_counts(), \n",
    "    7: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 7]['end_station_name'].value_counts(), \n",
    "    8: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 8]['end_station_name'].value_counts(), \n",
    "    9: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 9]['end_station_name'].value_counts(), \n",
    "    10: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 10]['end_station_name'].value_counts(), \n",
    "    11: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 11]['end_station_name'].value_counts(), \n",
    "    12: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 12]['end_station_name'].value_counts(), \n",
    "    13: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 13]['end_station_name'].value_counts(), \n",
    "    14: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 14]['end_station_name'].value_counts(),\n",
    "    15: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 15]['end_station_name'].value_counts(), \n",
    "    16: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 16]['end_station_name'].value_counts(), \n",
    "    17: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 17]['end_station_name'].value_counts(),\n",
    "    18: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 18]['end_station_name'].value_counts(), \n",
    "    19: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 19]['end_station_name'].value_counts(), \n",
    "    20: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 20]['end_station_name'].value_counts(), \n",
    "    21: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 21]['end_station_name'].value_counts(),\n",
    "    22: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 22]['end_station_name'].value_counts(), \n",
    "    23: boston_2018_trip_geodata_clean[boston_2018_trip_geodata_clean[\"hour\"] == 23]['end_station_name'].value_counts() \n",
    "})\n",
    "\n",
    "freq_z = freq_z.fillna(0)\n",
    "freq_z.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum \n",
    "most_pop_e = np.empty(24)\n",
    "freq_z['sum'] = 0\n",
    "\n",
    "for j in range(246):\n",
    "    for i in range(24):\n",
    "        most_pop_e[i] = freq_z[i][j]\n",
    "    freq_z['sum'][j] = most_pop_e.sum()\n",
    "\n",
    "freq_z.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the most popular\n",
    "most_freq_z = freq_z[freq_z['sum'] > 15000]\n",
    "most_freq_z.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure2, axes = plt.subplots(24, 1, figsize=(150, 200))\n",
    "\n",
    "sns.barplot(x = most_freq_z.index, y=0, data=most_freq_z, ax=axes[0], palette=\"OrRd\")\n",
    "axes[0].set_title(\"Most popular end stations\")\n",
    "sns.barplot(x = most_freq_z.index , y=1, data=most_freq_z, ax=axes[1], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=2, data=most_freq_z, ax=axes[2], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=3, data=most_freq_z, ax=axes[3], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=4, data=most_freq_z, ax=axes[4], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=5, data=most_freq_z, ax=axes[5], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=6, data=most_freq_z, ax=axes[6], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=7, data=most_freq_z, ax=axes[7], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=8, data=most_freq_z, ax=axes[8], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=9, data=most_freq_z, ax=axes[9], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=10, data=most_freq_z, ax=axes[10], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=11, data=most_freq_z, ax=axes[11], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=12, data=most_freq_z, ax=axes[12], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=13, data=most_freq_z, ax=axes[13], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=14, data=most_freq_z, ax=axes[14], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=15, data=most_freq_z, ax=axes[15], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=16, data=most_freq_z, ax=axes[16], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=17, data=most_freq_z, ax=axes[17], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=18, data=most_freq_z, ax=axes[18], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=19, data=most_freq_z, ax=axes[19], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=20, data=most_freq_z, ax=axes[20], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=21, data=most_freq_z, ax=axes[21], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=22, data=most_freq_z, ax=axes[22], palette=\"OrRd\")\n",
    "sns.barplot(x = most_freq_z.index , y=23, data=most_freq_z, ax=axes[23], palette=\"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine most popular start and end station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(246):\n",
    "    for i in range(24):\n",
    "        freq_h[i][j]=freq_h[i][j]+freq_z[i][j]\n",
    "freq_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum \n",
    "most_pop_e = np.empty(24)\n",
    "freq_h['sum'] = 0\n",
    "\n",
    "for j in range(246):\n",
    "    for i in range(24):\n",
    "        most_pop_e[i] = freq_h[i][j]\n",
    "    freq_h['sum'][j] = most_pop_e.sum()\n",
    "freq_h.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_h = freq_h[freq_h[\"sum\"]>30000]\n",
    "freq_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each barplot represents an hour. On the x-axis are the different most popular stations. Zoom in to read the axis labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(24, 1, figsize=(150, 200))\n",
    "\n",
    "sns.barplot(x = freq_h.index, y=0, data=freq_h, ax=axes[0], palette=\"OrRd\")\n",
    "axes[0].set_title(\"Most popular stations\")\n",
    "sns.barplot(x = freq_h.index , y=1, data=freq_h, ax=axes[1], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=2, data=freq_h, ax=axes[2], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=3, data=freq_h, ax=axes[3], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=4, data=freq_h, ax=axes[4], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=5, data=freq_h, ax=axes[5], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=6, data=freq_h, ax=axes[6], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=7, data=freq_h, ax=axes[7], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=8, data=freq_h, ax=axes[8], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=9, data=freq_h, ax=axes[9], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=10, data=freq_h, ax=axes[10], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=11, data=freq_h, ax=axes[11], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=12, data=freq_h, ax=axes[12], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=13, data=freq_h, ax=axes[13], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=14, data=freq_h, ax=axes[14], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=15, data=freq_h, ax=axes[15], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=16, data=freq_h, ax=axes[16], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=17, data=freq_h, ax=axes[17], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=18, data=freq_h, ax=axes[18], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=19, data=freq_h, ax=axes[19], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=20, data=freq_h, ax=axes[20], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=21, data=freq_h, ax=axes[21], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=22, data=freq_h, ax=axes[22], palette=\"OrRd\")\n",
    "sns.barplot(x = freq_h.index , y=23, data=freq_h, ax=axes[23], palette=\"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the most popular stations overall \n",
    "On this map you can see the most popular stations overall. The stations are represented by means of markers. You can click on a marker to see the name of the station. Switch on the tilelayer 'openstreetmap' to identify interesting places like an  university nearby a station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the most popular stations overall\n",
    "overall = pd.DataFrame({\n",
    "    \"Anzahl\": freq_h[\"sum\"]\n",
    "})\n",
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pop_stations = folium.Map(location=(42.337510, -71.067966), zoom_start=12)\n",
    "\n",
    "boston_and_greater = r'boston_and_greater.json'\n",
    "# Boston = r'ZIP_Codes.geojson'\n",
    "# Cambridge = r'Cambridge_Zipcodes.geojson'\n",
    "# Somerville = r'somerville.json'\n",
    "# Brookline = r'brookline.json'\n",
    "\n",
    "#add station markers\n",
    "folium.Marker(\n",
    "    [42.3625, -71.08822], popup=\"<i>Ames St at Main St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.34807412, -71.07657015], popup=\"<i>Back Bay T Stop - Dartmouth St at Stuart St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.35082681, -71.08981088], popup=\"<i>Beacon St at Massachusetts Ave</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.35892, -71.08981088], popup=\"<i>Boston City Hall - 28 State St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.36125722, -71.06528744], popup=\"<i>Cambridge St at Joy St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.366426, -71.105495], popup=\"<i>Central Sq Post Office / Cambridge City Hall at Mass Ave / Pleasant St\t</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.36507, -71.1031], popup=\"<i>Central Square at Mass Ave / Essex St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.36079297, -71.07118962], popup=\"<i>Charles Circle - Charles St at Cambridge St\t</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.34366582, -71.08582377], popup=\"<i>Christian Science Plaza - Massachusetts Ave at Westland Ave</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.34992828, -71.07739207], popup=\"<i>Copley Square - Dartmouth St at Boylston St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.362811, -71.056067], popup=\"<i>Cross St at Hanover St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.373268, -71.118579], popup=\"<i>Harvard Square at Mass Ave/ Dunster</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.36242784, -71.08495474], popup=\"<i>Kendall T</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.348706, -71.097009], popup=\"<i>Kenmore Square</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.370677, -71.076529], popup=\"<i>Lechmere Station at Cambridge St / First St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.3595732, -71.10129476], popup=\"<i>MIT Pacific St at Purrington St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.36213123, -71.09115601], popup=\"<i>MIT Stata Center at Vassar St / Main St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.35560121, -71.10394478], popup=\"<i>MIT Vassar St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.3581, -71.093198], popup=\"<i>MIT at Mass Ave / Amherst St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.365673, -71.064263], popup=\"<i>Nashua Street at Red Auerbach Way</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.348717, -71.085954], popup=\"<i>Newbury St at Hereford St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.366277, -71.09169], popup=\"<i>One Kendall Square at Hampshire St / Portland St</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.352175, -71.055547], popup=\"<i>South Station - 700 Atlantic Ave</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "folium.Marker(\n",
    "    [42.36264779, -71.10006094], popup=\"<i>University Park</i>\"\n",
    ").add_to(most_pop_stations)\n",
    "\n",
    "#add layers\n",
    "most_pop_stations.choropleth(\n",
    "    name=\"Greater Boston\",\n",
    "    geo_data='boston_and_greater.json',\n",
    "    fill_color=\"grey\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=1.0\n",
    ")\n",
    "most_pop_stations.choropleth(\n",
    "    name=\"Boston\",\n",
    "    geo_data='ZIP_Codes.geojson',\n",
    "    fill_color=\"blue\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=1.0\n",
    ")\n",
    "most_pop_stations.choropleth(\n",
    "    name=\"Cambridge\",\n",
    "    geo_data='Cambridge_Zipcodes.geojson',\n",
    "    fill_color=\"red\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=1.0\n",
    ")\n",
    "most_pop_stations.choropleth(\n",
    "    name=\"Somerville\",\n",
    "    geo_data='somerville.json',\n",
    "    fill_color=\"green\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=1.0\n",
    ")\n",
    "most_pop_stations.choropleth(\n",
    "    name=\"Brookline\",\n",
    "    geo_data='brookline.json',\n",
    "    fill_color=\"purple\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=1.0\n",
    ")\n",
    "\n",
    "\n",
    "#add tilelayers \n",
    "folium.TileLayer('cartodbpositron').add_to(most_pop_stations)\n",
    "folium.TileLayer('Stamen Terrain').add_to(most_pop_stations)\n",
    "folium.TileLayer('Openstreetmap').add_to(most_pop_stations)\n",
    "folium.LayerControl().add_to(most_pop_stations)\n",
    "               \n",
    "display(most_pop_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Predictive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the predictive analytics, we will first need to import some modules from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to extract the features we have selected in **3.1 Feature Engineering** in our report.\n",
    "These are the month, weekday, hour, max temperature (or min temperature, just one temperature referece) and precipitation.\n",
    "\n",
    "As we want to predict hourly values, we also have to extract these values and possibly manipulate them to be hourly.\n",
    "We do that by taking the median of the features needed to be manipulated, so the median of max_temp and precip.\n",
    "\n",
    "We then take a look at our new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_hourly_data = boston_2018_trip_geodata_with_temp_clean.groupby([\"month\", \"weekday\", \"hour\"]).median()[[\"max_temp\", \"precip\"]]\n",
    "grouped_hourly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now have our features, we will need to create the dependent variable, so the data we want to predict with our features. For that we choose the demand of bikes per hour, so the amount of trips started in an hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_amount_data = boston_2018_trip_geodata_with_temp_clean.groupby([\"month\", \"weekday\", \"hour\"]).count()[\"start_time\"]\n",
    "grouped_amount_data.rename(\"amount\", inplace=True)\n",
    "grouped_amount_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now combine these two datasets into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped_data = pd.concat([grouped_hourly_data, grouped_amount_data], axis=1)\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the data, we notice that our dataset has three indices. To better work with it, we reset it to a simple\n",
    "integer index counting the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_data = grouped_data.reset_index()\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split our data up into the x values, which are the features, and the y values, which is the value to be predicted.\n",
    "\n",
    "So the x values are the independent variables and the y values are the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_entire = grouped_data[[\"weekday\", \"hour\", \"month\", \"max_temp\", \"precip\"]]\n",
    "y_entire = grouped_data[\"amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a standardized and scaled version of the x values with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "x_entire_norm = norm.fit_transform(x_entire)\n",
    "x_entire_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have to decide on a type of regression, we want to take a look at relationships between the features and the variable to predict. We do that by plotting some of them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "# Plotting the data\n",
    "ax.scatter(grouped_data[\"hour\"].values, grouped_data[\"amount\"].values, marker='x')\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Amount of bikes used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "# Plotting the data\n",
    "ax.scatter(grouped_data[\"month\"], grouped_data[\"amount\"], marker='x',)\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Amount of bikes used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "# Plotting the data\n",
    "ax.scatter(grouped_data[\"max_temp\"], grouped_data[\"amount\"], marker='x',)\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Amount of bikes used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the temperature data seems to have somewhat of a linear relationship with the amount, especially the temporal data is pretty obviously not linear. But if we put all these features together, what kind of relationship will come out? We will take that into account when deciding our selection of three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use this dataset for regression, we first have to split it up in a training set, a holdout set and a validation set.\n",
    "\n",
    "A **training set** (50% of the data) to train the initial model, a **holdout set** (20% of the data) to adjust the hyperparameters and then the **validation set** (30& of the data) to finally evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n_full, x_test_n, y_train_n_full, y_test_n = train_test_split(x_entire_norm, y_entire, test_size=0.3,random_state=40)\n",
    "x_train_n, x_holdout_n, y_train_n, y_holdout_n = train_test_split(x_train_n_full, y_train_n_full, test_size = 0.2/0.7, random_state=40)\n",
    "\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(x_entire, y_entire, test_size=0.3,random_state=40)\n",
    "x_train, x_holdout, y_train, y_holdout = train_test_split(x_train_full, y_train_full, test_size = 0.2/0.7, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel in SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svr = svm.SVR(kernel=\"rbf\", C=100.0, coef0=1.0)\n",
    "model_svr.fit(x_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model_svr.predict(x_test_n)\n",
    "\n",
    "mae = mean_absolute_error(y_pred=y_predict, y_true=y_test_n)\n",
    "mse = mean_squared_error(y_pred=y_predict, y_true=y_test_n)\n",
    "determination = r2_score(y_pred=y_predict, y_true=y_test_n)\n",
    "print(\"Mean absolute error: \", mae)\n",
    "print(\"Mean squared error: \", mse)\n",
    "print(\"R² = \", determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the `KNeighborsRegressor` of sklearn with the default parameters.\n",
    "As the k-nearest-neighbor regression algorithm needs normalized and scaled data, we enter our normalized and scaled\n",
    "training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_reg = KNeighborsRegressor(n_neighbors=5, leaf_size=30, p=2)\n",
    "KNN_model = KNN_reg.fit(x_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test this regression algorithm on the test dataset to see its initial performance on some evaluation metrics. There is an explanation in the report for why we chose those metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = KNN_model.predict(x_test_n)\n",
    "\n",
    "mae = mean_absolute_error(y_pred=y_predict, y_true=y_test_n)\n",
    "mse = mean_squared_error(y_pred=y_predict, y_true=y_test_n)\n",
    "rmse = mse**0.5\n",
    "print(\"Mean absolute error: \", mae)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Root mean squared error: \", rmse)\n",
    "print(\"Average error:\", np.average(y_test_n - y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, but is this the best we can get?\n",
    "\n",
    "To find out, we will perform a **grid search** on our holdout dataset to select the optimal hyperparameters for the model. We do that similarly for every parameter. We first go through a range of values and set the hyperparameter to each value. hen the results are plotted and we take a look at how the mean squared error and the average error developed in this value range. Finally we usually just look for the lowest y value in the plot and take that x value as a hyperparameter.\n",
    "\n",
    "This has to be done on the holdout dataset and not on the test dataset, as otherwise it would cause data leakage due to us selecting the best value on the results itself, which distorts the actual performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will try the **n_neighbors** hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse = np.zeros(20)\n",
    "results_avge = np.zeros(20)\n",
    "for i in range(20):\n",
    "    KNN_reg = KNeighborsRegressor(n_neighbors=i + 1)\n",
    "    KNN_model = KNN_reg.fit(x_train_n, y_train_n)\n",
    "    y_predict = KNN_model.predict(x_holdout_n)\n",
    "    mse = mean_squared_error(y_pred=y_predict, y_true=y_holdout_n)\n",
    "    avge = np.average(y_holdout_n - y_predict)\n",
    "    results_mse[i] = mse\n",
    "    results_avge[i] = avge\n",
    "    \n",
    "plt.plot(np.arange(1, 21), results_mse)\n",
    "plt.show()\n",
    "plt.plot(np.arange(1,21), results_avge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a value between 3 and 8 is the optimal range for **n_neighbors**. As the y value is lowest around 4, we take that value as our hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do the same with the **leaf_size** hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse = np.zeros(50)\n",
    "results_avge = np.zeros(50)\n",
    "for i in range(50):\n",
    "    KNN_reg = KNeighborsRegressor(n_neighbors=4, leaf_size=i + 1)\n",
    "    KNN_model = KNN_reg.fit(x_train_n, y_train_n)\n",
    "    y_predict = KNN_model.predict(x_holdout_n)\n",
    "    mse = mean_squared_error(y_pred=y_predict, y_true=y_holdout_n)\n",
    "    avge = np.average(y_holdout_n - y_predict)\n",
    "    results_mse[i] = mse\n",
    "    results_avge[i] = avge\n",
    "    \n",
    "plt.plot(np.arange(1, 51), results_mse)\n",
    "plt.show()\n",
    "plt.plot(np.arange(1, 51), results_avge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to affect the mean squared error much, so we will stay with the default value for **leaf_size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we do the same with hyperparameter **p**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse = np.zeros(10)\n",
    "results_avge = np.zeros(10)\n",
    "for i in range(10):\n",
    "    KNN_reg = KNeighborsRegressor(n_neighbors=4, leaf_size=30, p=i+1)\n",
    "    KNN_model = KNN_reg.fit(x_train_n, y_train_n)\n",
    "    y_predict = KNN_model.predict(x_holdout_n)\n",
    "    mse = mean_squared_error(y_pred=y_predict, y_true=y_holdout_n)\n",
    "    avge = np.average(y_holdout_n - y_predict)\n",
    "    results_mse[i] = mse\n",
    "    results_avge[i] = avge\n",
    "    \n",
    "plt.plot(np.arange(1, 11), results_mse)\n",
    "plt.show()\n",
    "plt.plot(np.arange(1,11), results_avge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value already seems to be the best choice, so we will stay with that value as our hyperparameter **p** for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to evaluating the model with the new hyperparameters, we visualize the mean squared error and the average error for the training, holdout and validation dataset across the **n_neigbors** hyperparameter to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse_train = np.zeros(20)\n",
    "results_mse_test = np.zeros(20)\n",
    "results_mse_holdout = np.zeros(20)\n",
    "results_avge_train = np.zeros(20)\n",
    "results_avge_test = np.zeros(20)\n",
    "results_avge_holdout = np.zeros(20)\n",
    "for i in range(20):\n",
    "    KNN_reg = KNeighborsRegressor(n_neighbors=i + 1)\n",
    "    KNN_model = KNN_reg.fit(x_train_n, y_train_n)\n",
    "    y_predict_train = KNN_model.predict(x_train_n)\n",
    "    y_predict_test = KNN_model.predict(x_test_n)\n",
    "    y_predict_holdout = KNN_model.predict(x_holdout_n)\n",
    "    \n",
    "    results_mse_train[i] = mean_squared_error(y_pred=y_predict_train, y_true=y_train_n)\n",
    "    results_mse_test[i] = mean_squared_error(y_pred=y_predict_test, y_true=y_test_n)\n",
    "    results_mse_holdout[i] = mean_squared_error(y_pred=y_predict_holdout, y_true=y_holdout_n)\n",
    "    results_avge_train[i] = np.average(y_train_n - y_predict_train)\n",
    "    results_avge_test[i] = np.average(y_test_n - y_predict_test)\n",
    "    results_avge_holdout[i] = np.average(y_holdout_n - y_predict_holdout)\n",
    "    \n",
    "plt.plot(np.arange(1, 21), results_mse_train, label=\"Train\")\n",
    "plt.plot(np.arange(1, 21), results_mse_test, label=\"Test\")\n",
    "plt.plot(np.arange(1, 21), results_mse_holdout, label=\"Holdout\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "plt.plot(np.arange(1,21), results_avge_train, label=\"Train\")\n",
    "plt.plot(np.arange(1,21), results_avge_test, label=\"Test\")\n",
    "plt.plot(np.arange(1,21), results_avge_holdout, label=\"Holdout\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate our dataset again.\n",
    "\n",
    "We begin by creating a new model `KNeighborsRegressor` model with the new hyperparameters. We fit in the training dataset, but also the holdout dataset into this new model. Then, we let it predict the values of the test dataset and print out the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_reg = KNeighborsRegressor(n_neighbors=4, leaf_size=30, p=2)\n",
    "KNN_model = KNN_reg.fit(x_train_n_full, y_train_n_full)\n",
    "\n",
    "y_predict = KNN_model.predict(x_test_n)\n",
    "\n",
    "mae = mean_absolute_error(y_pred=y_predict, y_true=y_test_n)\n",
    "mse = mean_squared_error(y_pred=y_predict, y_true=y_test_n)\n",
    "rmse = mse**0.5\n",
    "print(\"Mean absolute error: \", mae)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Root mean squared error: \", rmse)\n",
    "print(\"Average error:\", np.average(y_test_n - y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the values have improved, even though the default values were already quite close to the values found with the grid search. We can now use these values to compare this model with our two other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_reg = DecisionTreeRegressor(max_depth=5)\n",
    "tree_model = Tree_reg.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = tree_model.predict(x_test)\n",
    "\n",
    "mae = mean_absolute_error(y_pred=y_predict, y_true=y_test)\n",
    "mse = mean_squared_error(y_pred=y_predict, y_true=y_test)\n",
    "determination = r2_score(y_pred=y_predict, y_true=y_test)\n",
    "rmse = (mean_squared_error(y_pred=y_predict, y_true=y_test))**(0.5)\n",
    "print(\"Mean absolute error: \", mae)\n",
    "print(\"Mean squared error: \", mse)\n",
    "print(\"Root mean squared error: \", rmse)\n",
    "#print(\"R² = \", determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tree_depth (x,y):\n",
    "    \n",
    "    # define list for collecting results\n",
    "    err_train = [] \n",
    "    err_test = []\n",
    "    \n",
    "    # split data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=10)\n",
    "    \n",
    "    #loop over max_depth\n",
    "    \n",
    "    for n in np.arange(1,21): # lets test until 24 for now\n",
    "        \n",
    "        # fit model\n",
    "        \n",
    "        tree_reg = DecisionTreeRegressor(max_depth=n)\n",
    "        tree_model = Tree_reg.fit(x_train, y_train)\n",
    "        \n",
    "        # compute errors\n",
    "        \n",
    "        err_train.append(mean_absolute_error(y_train, tree_model.predict(x_train)))\n",
    "        err_test.append(mean_absolute_error(y_test, tree_model.predict(x_test)))\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.plot(np.arange(1,21), err_train,np.arange(1,21), err_test)\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.xlabel(\"Max Tree Depth\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"Search over max_depth parameter\",fontsize=14)\n",
    "    #plt.ylim((0,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_tree_depth (x_entire,y_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rmse = np.zeros(20)\n",
    "results_avge = np.zeros(20)\n",
    "for i in range(20):\n",
    "    Tree_reg = DecisionTreeRegressor(max_depth=i + 1)\n",
    "    tree_model = Tree_reg.fit(x_train, y_train) \n",
    "    y_predict = tree_model.predict(x_holdout)\n",
    "    rmse = (mean_squared_error(y_pred=y_predict, y_true=y_holdout))**0.5\n",
    "    avge = np.average(y_holdout - y_predict)\n",
    "    results_rmse[i] = rmse\n",
    "    results_avge[i] = avge\n",
    "    \n",
    "plt.plot(np.arange(1, 21), results_rmse)\n",
    "plt.show()\n",
    "plt.plot(np.arange(1,21), results_avge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rmse = np.zeros(50)\n",
    "results_avge = np.zeros(50)\n",
    "for i in range(20):\n",
    "    Tree_reg = DecisionTreeRegressor(max_depth=i + 1)\n",
    "    tree_model = Tree_reg.fit(x_train, y_train) \n",
    "    y_predict = tree_model.predict(x_holdout)\n",
    "    rmse = (mean_squared_error(y_pred=y_predict, y_true=y_holdout))**0.5\n",
    "    avge = np.average(y_holdout - y_predict)\n",
    "    results_rmse[i] = rmse\n",
    "    results_avge[i] = avge\n",
    "    \n",
    "plt.plot(np.arange(1, 51), results_rmse)\n",
    "plt.show()\n",
    "plt.plot(np.arange(1,51), results_avge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rmse_train = np.zeros(20)\n",
    "results_rmse_test = np.zeros(20)\n",
    "results_rmse_holdout = np.zeros(20)\n",
    "results_avge_train = np.zeros(20)\n",
    "results_avge_test = np.zeros(20)\n",
    "results_avge_holdout = np.zeros(20)\n",
    "for i in range(20):\n",
    "    Tree_reg = DecisionTreeRegressor(max_depth=i + 1)\n",
    "    tree_model = Tree_reg.fit(x_train, y_train) \n",
    "    y_predict_train = tree_model.predict(x_train)\n",
    "    y_predict_test = tree_model.predict(x_test)\n",
    "    y_predict_holdout = tree_model.predict(x_holdout)\n",
    "    \n",
    "    results_rmse_train[i] = (mean_squared_error(y_pred=y_predict_train, y_true=y_train))**0.5\n",
    "    results_rmse_test[i] = (mean_squared_error(y_pred=y_predict_test, y_true=y_test))**0.5\n",
    "    results_rmse_holdout[i] = (mean_squared_error(y_pred=y_predict_holdout, y_true=y_holdout))**0.5\n",
    "    results_avge_train[i] = np.average(y_train_n - y_predict_train)\n",
    "    results_avge_test[i] = np.average(y_test_n - y_predict_test)\n",
    "    results_avge_holdout[i] = np.average(y_holdout - y_predict_holdout)\n",
    "    \n",
    "plt.plot(np.arange(1, 21), results_rmse_train, label=\"Train\")\n",
    "plt.plot(np.arange(1, 21), results_rmse_test, label=\"Test\")\n",
    "plt.plot(np.arange(1, 21), results_rmse_holdout, label=\"Holdout\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "plt.plot(np.arange(1,21), results_avge_train, label=\"Train\")\n",
    "plt.plot(np.arange(1,21), results_avge_test, label=\"Test\")\n",
    "plt.plot(np.arange(1,21), results_avge_holdout, label=\"Holdout\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_reg = DecisionTreeRegressor(max_depth=8)\n",
    "tree_model = Tree_reg.fit(x_train_full, y_train_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = tree_model.predict(x_test)\n",
    "\n",
    "mae = mean_absolute_error(y_pred=y_predict, y_true=y_test)\n",
    "mse = mean_squared_error(y_pred=y_predict, y_true=y_test)\n",
    "determination = r2_score(y_pred=y_predict, y_true=y_test)\n",
    "rmse = (mean_squared_error(y_pred=y_predict, y_true=y_test))**(0.5)\n",
    "print(\"Mean absolute error: \", mae)\n",
    "print(\"Mean squared error: \", mse)\n",
    "print(\"Root mean squared error: \", rmse)\n",
    "#print(\"R² = \", determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
